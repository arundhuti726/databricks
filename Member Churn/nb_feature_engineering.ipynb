{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c219b37e-24f3-49ec-b68e-0e37180aa22e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read the file from 'dbfs:/user/arundhuti/delta//member_churn/silver/' \n",
    "member_df = spark.read.format('delta').load('dbfs:/user/arundhuti/delta/member_churn/silver/')\n",
    "\n",
    "display(member_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50f9a273-7754-4317-8b53-27ff03aca853",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Transformaing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fc8a21f-106d-441c-87df-fd5ec6da024c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415997a3-b9d4-4c77-8cfc-8325b008fd1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "#create age bucket, like 18-30, 31-45, 46-60, 61-75, 76-90\n",
    "member_sil_df = member_df.withColumn(\n",
    "    'Age_Bucket', \n",
    "    F.when(F.col('Age') <= 30, '18-30')\n",
    "     .when(F.col('Age') <= 45, '31-45')\n",
    "     .when(F.col('Age') <= 60, '46-60')\n",
    "     .when(F.col('Age') <= 75, '61-75')\n",
    "     .when(F.col('Age') <= 90, '76-90')\n",
    ")\n",
    "#display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7057e084-2fbc-4bca-a078-603f2c116bd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # from pyspark.sql import functions as F\n",
    "\n",
    "member_sil_df = member_sil_df.withColumn('Gender_modified', F.when(F.col('Gender') == 'Male', 1)\n",
    "                              .when(F.col('Gender') == 'Female', 0)\n",
    "                              .otherwise(None)\n",
    "                            )\n",
    "\n",
    "\n",
    "display(member_sil_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e07ca0-c059-4070-9278-65c71537cd80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "gender_indexer = StringIndexer(inputCol='gender', outputCol='Gender_indexed', handleInvalid='skip')\n",
    "member_sil_df = gender_indexer.fit(member_sil_df).transform(member_sil_df)\n",
    "\n",
    "display(member_sil_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c9580b5-3d06-4f0a-b17b-74480ee1e501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Assemble the 'Total_Claim_Amount' into a vector column first\n",
    "assembler = VectorAssembler(inputCols=['Total_Claim_Amount'], outputCol='Total_Claim_Amount_vec')\n",
    "member_sil_df = assembler.transform(member_sil_df)\n",
    "\n",
    "# Apply StandardScaler on the vector column\n",
    "scaler = StandardScaler(inputCol='Total_Claim_Amount_vec', outputCol='Total_Claim_Amount_scaled', withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(member_sil_df)\n",
    "member_sil_df = scaler_model.transform(member_sil_df)\n",
    "\n",
    "# Extract the scaled value from the vector to a double column\n",
    "extract_value_udf = udf(lambda v: float(v[0]) if v is not None else None, DoubleType())\n",
    "member_sil_df = member_sil_df.withColumn('Total_Claim_Amount_scaled', extract_value_udf(col('Total_Claim_Amount_scaled')))\n",
    "\n",
    "# Drop the intermediate vector column\n",
    "member_sil_df = member_sil_df.drop('Total_Claim_Amount_vec')\n",
    "\n",
    "display(member_sil_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f198f8b6-a2b8-4847-9eb8-e76e26530e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# # get value from Total_Claim_Amount_scaled.values\n",
    "# #create a loop which will go 1 by 1 on each row and get the value and stored it in 'val' and the iterate that with df\n",
    "# for row in df.select('Total_Claim_Amount_scaled').collect():\n",
    "#     val = row['Total_Claim_Amount_scaled'][0]\n",
    "#     #print(val) \n",
    "#     df = df.withColumn('High_Risk_Flag', F.when(F.col('Total_Claim_Amount_scaled')[0] > 0.5, 1).otherwise(0))\n",
    "# display(df)   \n",
    "\n",
    "# # val = df.select('Total_Claim_Amount_scaled').collect()[0]['Total_Claim_Amount_scaled'][0]\n",
    "# # # Create a new feature : High_Risk_Flag based on the condition, and use the 'val' value from above\n",
    "# # df = df.withColumn('High_Risk_Flag', F.when(F.val > 0.5, 1).otherwise(0))\n",
    "# # display(df)\n",
    "\n",
    "# # from pyspark.sql import functions as F\n",
    "\n",
    "# # # Extract the 'values' field from the UDT and then get the first item\n",
    "# # df = df.withColumn(\n",
    "# #     'High_Risk_Flag',\n",
    "# #     F.when(F.col('Total_Claim_Amount_scaled').getField('values').getItem(0) > 0.5, 1).otherwise(0)\n",
    "# # )\n",
    "# # display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99d15d4f-4c4d-4e12-927b-72f0abff2a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Add High_Risk_Flag column\n",
    "member_sil_df = member_sil_df.withColumn(\n",
    "    \"High_Risk_Flag\",\n",
    "    when(\n",
    "        (col(\"Chronic_Conditions\") >= 2) |\n",
    "        (col(\"Premium_Increase_Pct\") >= 15) |\n",
    "        (col(\"Late_Payments\") >= 2) |\n",
    "        (col(\"Customer_Service_Calls\") >= 3),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Optional: Display result\n",
    "display(member_sil_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde1164a-5c1e-406e-aee3-bc70c938b81a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write the data as delta format in the folder 'dbfs:/user/arundhuti/delta/featured_data/' in change data capture mode\n",
    "member_sil_df.write.format('delta').mode('overwrite').option('mergeSchema', 'true').save('dbfs:/user/arundhuti/delta/member_churn/silver/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc58b9d6-17cd-42d9-8862-cd42a60edd2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--SELECT * FROM delta.`dbfs:/user/arundhuti/delta/member_churn/silver/`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5301096313901636,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
