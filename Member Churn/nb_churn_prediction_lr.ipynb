{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae17bd5f-e5cc-4683-8287-cf4f07cb23f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read the file from 'dbfs:/user/arundhuti/delta/featured_data' in cdc mode and create a dataframe to sore the value\n",
    "member_featured_df = spark.read.format('delta').load('dbfs:/user/arundhuti/delta/member_churn/silver/')\n",
    "display(member_featured_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d79cb9de-a5de-4631-b1da-9a908db5a903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split the data into training and test sets\n",
    "train_df, test_df = member_featured_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Convert Age_Bucket to numeric type\n",
    "indexer = StringIndexer(inputCol=\"Age_Bucket\", outputCol=\"Age_Bucket_indexed\")\n",
    "encoder = OneHotEncoder(inputCol=\"Age_Bucket_indexed\", outputCol=\"Age_Bucket_encoded\")\n",
    "\n",
    "feature = [\"Age_Bucket_encoded\", \"Gender_indexed\", \"Claim_count\", \"Tenure_Months\", \"Customer_Service_Calls\", \"Total_Claim_Amount_scaled\", \"High_Risk_Flag\"]\n",
    "assembler = VectorAssembler(inputCols=feature, outputCol=\"features\")\n",
    "\n",
    "label_indexer = StringIndexer(inputCol=\"Churned\", outputCol=\"Churned_indexed\")\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"Churned_indexed\", featuresCol=\"features\")\n",
    "\n",
    "pipeline1 = Pipeline(stages=[indexer, encoder, assembler, label_indexer, lr])\n",
    "\n",
    "# Fit the model on the training data\n",
    "model = pipeline1.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8098e217-9554-44f0-bee0-d3453ae06498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write the result in to the folder dbfs:/user/arundhuti/delta/model_data in to the delta format\n",
    "model.write().overwrite().save(\"dbfs:/user/arundhuti/delta/member_churn/model_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ef3a8d-a956-4a8f-9216-b18b67f1f932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load and use the saved pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_pipeline_model = PipelineModel.load(\"dbfs:/user/arundhuti/delta/member_churn/model_data\")\n",
    "\n",
    "# Show pipeline stages\n",
    "loaded_pipeline_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01c9b5f9-f1fd-4d97-b009-f02e95c1ac80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use the loaded pipeline to transform the test dataset\n",
    "test_transformed_df = loaded_pipeline_model.transform(test_df)\n",
    "\n",
    "display(test_transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "85d16ea6-9af4-4c54-81ee-879cb063677e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# # Read the test data\n",
    "# df = test_transformed_df\n",
    "\n",
    "# if df is not None:\n",
    "#     # Filter for high churn probability and select member IDs with reason of churn\n",
    "#     high_churn_members = df.where(col('high_Risk_flag') == 1.0).select('Member_ID', 'features','prediction')\n",
    "    \n",
    "#     # Extract feature values from the vector\n",
    "#     from pyspark.ml.functions import vector_to_array\n",
    "#     high_churn_members_extracted = high_churn_members.withColumn(\"features_array\", vector_to_array(col(\"features\")))\n",
    "    \n",
    "#     # Convert features_array to human readable format\n",
    "#     feature_columns = ['age', 'monthly_spend', 'tenure_months']  # replace with actual feature names\n",
    "#     for i, feature in enumerate(feature_columns):\n",
    "#         high_churn_members_extracted = high_churn_members_extracted.withColumn(feature, col(\"features_array\")[i])\n",
    "    \n",
    "#     display(high_churn_members_extracted.drop(\"features_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02b9d90-30d0-4a11-bc04-5087d7e67fa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import avg\n",
    "\n",
    "# # Calculate average values for high churn members\n",
    "# avg_values = high_churn_members_extracted.agg(\n",
    "#     avg('age').alias('avg_age'),\n",
    "#     avg('monthly_spend').alias('avg_monthly_spend'),\n",
    "#     avg('tenure_months').alias('avg_tenure_months')\n",
    "# ).collect()[0]\n",
    "\n",
    "# # Display the average values\n",
    "# display(avg_values)\n",
    "\n",
    "# # Conclusion based on the average values\n",
    "# conclusion = f\"\"\"\n",
    "# Based on the analysis of high churn members:\n",
    "# - The average age is {avg_values['avg_age']:.2f} years.\n",
    "# - The average monthly spend is ${avg_values['avg_monthly_spend']:.2f}.\n",
    "# - The average tenure is {avg_values['avg_tenure_months']:.2f} months.\n",
    "\n",
    "# These factors indicate that members with higher churn probability tend to have specific characteristics in terms of age, spending, and tenure.\n",
    "# \"\"\"\n",
    "\n",
    "# print(conclusion)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_churn_prediction_lr",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
