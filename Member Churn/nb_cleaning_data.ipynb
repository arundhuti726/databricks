{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d82d2ab6-3b39-4758-86b7-d99916121e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read the bronze data\n",
    "member_df = spark.read.format(\"delta\").load(\"dbfs:/user/arundhuti/delta/member_churn/bronze\")\n",
    "display(member_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3907fea3-2132-4c64-8a72-a3a27cd8ee88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# count total number of null records of each field and only show the name of the fields if the counts > 0\n",
    "null_counts = member_df.select([count(when(col(column).isNull(), column)).alias(column) for column in member_df.columns])\n",
    "non_zero_null_counts = [column for column in null_counts.columns if null_counts.collect()[0][column] > 0]\n",
    "display(null_counts.select(non_zero_null_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f82db4-e52a-4713-8a98-9933f23621e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Handling Null values and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbba4c94-0b3f-4bf1-afe5-f02cba0d88a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Replacing string \"null\" with actual NULL values\n",
    "for column in member_df.columns:\n",
    "  member_df = member_df.withColumn(column, when(col(column) == \"null\", None).otherwise(col(column)))\n",
    "\n",
    "member_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08cde906-50e3-4cbe-b399-92f808df3761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Find with Missing Values Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89bcecd5-6109-4b5c-9851-1c8825659759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify string columns\n",
    "string_cols = [c.name for c in member_df.schema.fields if c.dataType == StringType()]\n",
    "print(f\"String columns: {string_cols}\")\n",
    "\n",
    "# Count missing values in numeric columns\n",
    "string_missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in string_cols]\n",
    "row_dict_string = member_df.select(string_missing_values_logic).first().asDict()\n",
    "string_missing_cols = [column for column in row_dict_string if row_dict_string[column] > 0]\n",
    "\n",
    "print(f\"String columns with missing values: {string_missing_cols}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c46113c-189b-4e96-9359-449e471eab81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when, col\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# Identify boolean columns\n",
    "bool_cols = [c.name for c in member_df.schema.fields if c.dataType == BooleanType()]\n",
    "print(f\"Boolean columns: {bool_cols}\")\n",
    "\n",
    "# Count missing (null) values in boolean columns\n",
    "bool_missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in bool_cols]\n",
    "row_dict_bool = member_df.select(bool_missing_values_logic).first().asDict()\n",
    "bool_missing_cols = [column for column in row_dict_bool if row_dict_bool[column] > 0]\n",
    "\n",
    "print(f\"Boolean columns with missing values: {bool_missing_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b355b9-fe7e-48c8-bda7-6d0077cf7867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import mean, col, when, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Identify numeric columns for IntegerType\n",
    "int_cols = [c.name for c in member_df.schema.fields if c.dataType == IntegerType()]\n",
    "\n",
    "# Count missing values in numeric columns\n",
    "int_missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in int_cols]\n",
    "row_dict_int = member_df.select(int_missing_values_logic).first().asDict()\n",
    "int_missing_cols = [column for column in row_dict_int if row_dict_int[column] > 0]\n",
    "\n",
    "print(f\"Integer columns with missing values: {int_missing_cols}\")\n",
    "\n",
    "# Fill nulls with mean and cast to integer\n",
    "for column in int_missing_cols:\n",
    "    mean_value = member_df.select(mean(col(column))).first()[0]\n",
    "    member_df = member_df.withColumn(column + \"_filled\", col(column).cast(\"double\"))\n",
    "    member_df = member_df.fillna({column + \"_filled\": mean_value})\n",
    "    member_df = member_df.withColumn(column + \"_filled\", col(column + \"_filled\").cast(\"integer\"))\n",
    "\n",
    "# Drop the old fields\n",
    "member_df = member_df.drop(*int_missing_cols)\n",
    "\n",
    "# Rename the columns_filled to the old name columns\n",
    "# if the colum name contains _filled, then rename it to the old name\n",
    "member_df = member_df.drop(*int_missing_cols)\n",
    "for column in int_missing_cols:\n",
    "    member_df = member_df.withColumnRenamed(column + \"_filled\", column)\n",
    "\n",
    "\n",
    "display(member_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac15870-67fa-40d4-910a-491d3d74b2f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Adjusting Data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7492a6b8-31d4-47ee-a8d3-70af711e87a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write the cleaned data into the location '/dbfs/user/arundhuti/delta/customer_churn/silver/' from the train_df DataFrame\n",
    "member_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"dbfs:/user/arundhuti/delta/member_churn/silver/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e289f0-5107-42d1-bb83-24afccb37c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# splitting the data into train and test\n",
    "# train_df,test_df = member_df.randomSplit([0.8,0.2],seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31e36232-50c7-4929-b23b-09c5fa599631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %python\n",
    "# # write the trained and test data into the location '/dbfs/user/arundhuti/delta/customer_churn/silver/train_data/' from the train_df DataFrame\n",
    "# train_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"dbfs:/user/arundhuti/delta/member_churn/silver/train_data\")\n",
    "# test_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"dbfs:/user/arundhuti/delta/member_churn/silver/test_data\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_cleaning_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
