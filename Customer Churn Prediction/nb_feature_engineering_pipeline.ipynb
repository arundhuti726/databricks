{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0840180a-0c14-495f-85c2-c0d90458dc81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Generating Embedding for Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7557201d-5c45-4eed-9e32-395128c6b4aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import split, concat_ws, col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, Word2Vec\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType, DoubleType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def generate_categorical_embeddings(df, categorical_cols, vector_size=5):\n",
    "    \"\"\"\n",
    "    Generates embeddings for categorical columns using Word2Vec.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Input Spark DataFrame.\n",
    "    - categorical_cols (list): List of categorical column names.\n",
    "    - vector_size (int): Size of the embedding vectors.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with embeddings as a single vector columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace NULL categorical values with \"unknown\"\n",
    "    for col_name in categorical_cols:\n",
    "        df = df.withColumn(col_name, F.when(F.col(col_name).isNull(), 'unknown').otherwise(F.col(col_name)))\n",
    "\n",
    "    # Combine all categorical columns into a single column for Word2Vec\n",
    "    df = df.withColumn('categorical_seq', concat_ws(',', *categorical_cols))\n",
    "\n",
    "    # Tokenize categorical data\n",
    "    df = df.withColumn('categorical_tokens', split(col('categorical_seq'), ' '))\n",
    "\n",
    "    # Train Word2Vec model\n",
    "    word2vec = Word2Vec(vectorSize=vector_size, minCount=0, inputCol='categorical_tokens', outputCol='embeddings_struct')\n",
    "    model = word2vec.fit(df)\n",
    "    df = model.transform(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e06c7b48-72f9-46ea-9d2b-a9553a8e78be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Applying Embedding to a Categorical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5316ad3b-39ea-47a5-ac2d-1cbfb9c318ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the train_df from the location '/dbfs/user/arundhuti/delta/customer_churn/silver/train_data/'\n",
    "train_df = spark.read.format(\"delta\").load(\"dbfs:/user/arundhuti/delta/customer_churn/silver/train_data/\")\n",
    "test_df = spark.read.format(\"delta\").load(\"dbfs:/user/arundhuti/delta/customer_churn/silver/test_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f8bd76-46f7-42ec-8bdc-af515b7a0bdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Categorical columns\n",
    "categorical_cols = ['gender', 'Partner', 'InternetService', 'Contract', 'PaperlessBilling', 'PaymentMethod','churn'] \n",
    "\n",
    "# Generate embedding for categorical columns\n",
    "train_df = generate_categorical_embeddings(train_df, categorical_cols)\n",
    "test_df = generate_categorical_embeddings(test_df, categorical_cols)\n",
    "\n",
    "#display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be1e3788-49b0-48a8-8801-7d4df5e1e33b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Converting Embedding into Dense Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879c0c1a-f922-4ea3-b32f-ba685fc403e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract the 'values' field from the Word2Vec struct\n",
    "DenseVector_udf =  F.udf(lambda v: DenseVector(v.values) if v else DenseVector([0.0]*5), VectorUDT())\n",
    "\n",
    "# Convert embeddings into DenseVectors\n",
    "for col_name in categorical_cols:\n",
    "    train_df = train_df.withColumn(col_name + '_embeddings', DenseVector_udf(F.col('embeddings_struct')))\n",
    "    test_df = test_df.withColumn(col_name + '_embeddings', DenseVector_udf(F.col('embeddings_struct')))\n",
    "\n",
    "# Drop unnecerrary columns after embeddings are extracted    \n",
    "train_df = train_df.drop('categorical_seq', 'categorical_tokens', 'embeddings_struct')\n",
    "test_df = test_df.drop('categorical_seq', 'categorical_tokens', 'embeddings_struct')\n",
    "\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a77d9b9b-7577-4574-8a0f-628184165b0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show a sample of embedded categorical features\n",
    "display(train_df.select('gender_embeddings', 'Partner_embeddings', 'InternetService_embeddings', 'Contract_embeddings', 'PaperlessBilling_embeddings', 'PaymentMethod_embeddings', 'churn_embeddings'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1591130-5182-48a4-96da-d53bac524d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "870ab5d4-b387-41e3-aa89-beb2ac2ac54f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, Imputer , StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define numerical columns for imputation\n",
    "numerical_cols = [\"SeniorCitizen\", \"tenure\", \"TotalCharges\"]\n",
    "\n",
    "# Impute missing numerical features\n",
    "imputer = Imputer(inputCols=numerical_cols, outputCols=[col + \"_imputed\" for col in numerical_cols])\n",
    "\n",
    "# Assemble numerical columns into a single vector\n",
    "numerical_assembler = VectorAssembler(inputCols=[col + \"_imputed\" for col in numerical_cols], outputCol=\"numerical_assembled\")\n",
    "\n",
    "# Scale numerical features to standardize values\n",
    "numerical_scaler = StandardScaler(inputCol=\"numerical_assembled\", outputCol=\"numerical_scaled\")\n",
    "\n",
    "# Assemble all features (numerical+categorical embeddings) into a single vector\n",
    "feature_cols = [\"numerical_scaled\"] + [col + \"_embeddings\" for col in categorical_cols]\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"all_features\")\n",
    "\n",
    "# Define the sequence of transformations\n",
    "stage_list = [imputer, numerical_assembler, numerical_scaler, vector_assembler]\n",
    "\n",
    "# Instantiate the pipeline\n",
    "pipeline = Pipeline(stages=stage_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb54093-d55b-4c08-a5ab-838f6854abbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit the Pipeline \n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c151533-e501-4544-affc-e1e791225344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply the Feature Engineering Pipeline using .transform()\n",
    "train_transformed_df = pipeline_model.transform(train_df)\n",
    "test_transformed_df = pipeline_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94df16e-18f8-41c7-a3d3-f68ee63cf83c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show transformed features\n",
    "train_transformed_df.select('all_features').show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bfc4181-c04a-4634-94cf-885bae4de5eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Save and Reuse the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "558640de-f4b7-4bed-834b-552a015ed359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the Pipeline model with overwrite mode\n",
    "pipeline_model.write().overwrite().save(\"dbfs:/user/arundhuti/delta/customer_churn/silver/pipeline_model\")\n",
    "display(dbutils.fs.ls(\"dbfs:/user/arundhuti/delta/customer_churn/silver/pipeline_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c625f2-7ccf-476e-b26f-f7039d6ed6fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load and use the saved pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_pipeline_model = PipelineModel.load(\"dbfs:/user/arundhuti/delta/customer_churn/silver/pipeline_model\")\n",
    "\n",
    "# Show pipeline stages\n",
    "loaded_pipeline_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "717d2f68-4150-4418-8887-ca2f0381f0e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use the loaded pipeline to transform the test dataset\n",
    "test_transformed_df = loaded_pipeline_model.transform(test_df)\n",
    "\n",
    "display(test_transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef70c0c7-cf23-4b13-9915-5b4ba594f9fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registered the ML model\n",
    "from pyspark.ml.feature import Word2VecModel\n",
    "\n",
    "\n",
    "spark.udf.register(\"get_embeddings\", get_embeddings)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_feature_engineering_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
