{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d82d2ab6-3b39-4758-86b7-d99916121e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read the bronze data\n",
    "telco_df = spark.read.format(\"delta\").load(\"dbfs:/user/arundhuti/delta/customer_churn/bronze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f82db4-e52a-4713-8a98-9933f23621e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Handling Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbba4c94-0b3f-4bf1-afe5-f02cba0d88a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Replacing string \"null\" with actual NULL values\n",
    "for column in telco_df.columns:\n",
    "  telco_df = telco_df.withColumn(column, when(col(column) == \"null\", None).otherwise(col(column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b209cfb8-9aa8-4ccb-815b-739ca99d8bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Converting Data Type\n",
    "telco_df = (telco_df.withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(\"double\"))\n",
    "                    .withColumn(\"SeniorCitizen\", when(col(\"SeniorCitizen\")==1,True).otherwise(False))\n",
    "                    ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e289f0-5107-42d1-bb83-24afccb37c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# splitting the data into train and test\n",
    "train_df,test_df = telco_df.randomSplit([0.8,0.2],seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9f4240-060b-43a4-916f-8c105ea2182f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Transformaing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc92f46-c5b3-42ce-b75f-dba4bc0e8a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, BooleanType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, when, count\n",
    "# Get a list of integer & boolean columns\n",
    "int_cols = [column.name for column in train_df.schema.fields if (column.dataType == IntegerType() or column.dataType == BooleanType())]\n",
    "\n",
    "# Loop through integer columns and convert to double\n",
    "for column in int_cols:\n",
    "  train_df = train_df.withColumn(column, col(column).cast(\"double\"))\n",
    "  test_df = test_df.withColumn(column, col(column).cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08cde906-50e3-4cbe-b399-92f808df3761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Find Numeric columns with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c46113c-189b-4e96-9359-449e471eab81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,when\n",
    "# Identify numeric columns\n",
    "num_cols = [c.name for c in train_df.schema.fields if c.dataType == DoubleType()]\n",
    "\n",
    "# Count missing values in numeric columns\n",
    "num_missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in num_cols]\n",
    "row_dict_num = train_df.select(num_missing_values_logic).first().asDict()\n",
    "num_missing_cols = [column for column in row_dict_num if row_dict_num[column] > 0]\n",
    "\n",
    "print(f\"Numeric columns with missing values: {num_missing_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b355b9-fe7e-48c8-bda7-6d0077cf7867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify numeric columns for IntegerType\n",
    "int_cols = [c.name for c in train_df.schema.fields if c.dataType == IntegerType()]\n",
    "\n",
    "# Count missing values in numeric columns\n",
    "int_missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in int_cols]\n",
    "row_dict_int = train_df.select(int_missing_values_logic).first().asDict()\n",
    "int_missing_cols = [column for column in row_dict_int if row_dict_int[column] > 0]\n",
    "\n",
    "print(f\"Integer columns with missing values: {int_missing_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89bcecd5-6109-4b5c-9851-1c8825659759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify string columns\n",
    "string_cols = [c.name for c in train_df.schema.fields if c.dataType == StringType()]\n",
    "\n",
    "# Count missing values in numeric columns\n",
    "string_missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in string_cols]\n",
    "row_dict_string = train_df.select(string_missing_values_logic).first().asDict()\n",
    "string_missing_cols = [column for column in row_dict_string if row_dict_string[column] > 0]\n",
    "\n",
    "print(f\"String columns with missing values: {string_missing_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31e36232-50c7-4929-b23b-09c5fa599631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# write the trained and test data into the location '/dbfs/user/arundhuti/delta/customer_churn/silver/train_data/' from the train_df DataFrame\n",
    "train_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"dbfs:/user/arundhuti/delta/customer_churn/silver/train_data\")\n",
    "test_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"dbfs:/user/arundhuti/delta/customer_churn/silver/test_data\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_cleaning_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
