{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8348306e-0b01-4c52-9dff-815cc0214f8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./nb_utility_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f97a3549-a97d-4b7f-a73b-68484a38266d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql('USE CATALOG dev_edh')\n",
    "spark.sql('USE SCHEMA dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4fe8db-add9-4d4c-8094-ebb4276973a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# select current_catalog() , current_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f84cb7-ff2c-46ab-a3c8-c95c674d756d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--select * from sample_lab_data where test_name is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c5984e-e4a0-43f7-a29b-688e9e609b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lab_name = dbutils.jobs.taskValues.get(\"silver\", \"Lab_name\",debugValue=\"unknown\")\n",
    "# print(Lab_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d1108f-c56a-44a9-a0b6-1d2965e45567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lab = dbutils.jobs.taskValues.get(taskKey = \"get_Lab_Name\", key = \"Lab_name\", debugValue = \"Lab A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b63c6c-ba67-4599-a2c2-9cfa8c054f8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, date_format, to_timestamp\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "df = spark.table('sample_lab_data').where(col(\"test_source\") == lab)\n",
    "#display(df)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Check for null values in any field\n",
    "    null_df = checking_null(df, [\"member_id\", \"test_source\", \"test_name\", \"patient_city\", \"patient_state\", \"test_date\", \"patient_phone\", \"created_date\"])\n",
    "    #display(null_df)\n",
    "\n",
    "    # Save records with null values to 'quarantine' table\n",
    "    if null_df.count() > 0:\n",
    "        null_df.write.mode(\"overwrite\").saveAsTable(\"lab_data_quarantine\")\n",
    "    \n",
    "    # Filter out records with null values\n",
    "    cleaned_df = remove_null(df, [\"member_id\", \"test_source\", \"test_name\", \"patient_city\", \"patient_state\", \"test_date\", \"patient_phone\", \"created_date\"])\n",
    "    #display(cleaned_df)\n",
    "    \n",
    "    # Change the date data type to yyyy-MM-dd\n",
    "    cleaned_df = ( cleaned_df\n",
    "                           .withColumn(\"test_date\", date_format(col(\"test_date\"), \"yyyy-MM-dd\"))\n",
    "                           #.withColumn(\"created_date\", date_format(col(\"created_date\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "                           .withColumn(\"created_date\", date_format(to_timestamp(col(\"created_date\"), \"M/d/yyyy H:mm\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "                 )\n",
    "    \n",
    "    # Drop duplicate records\n",
    "    cleaned_df = cleaned_df.dropDuplicates()\n",
    "    \n",
    "    #display(cleaned_df)\n",
    "    \n",
    "except AnalysisException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a85035a7-7e7c-456c-822b-717b16d5d8e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "silver_df = cleaned_df.select(\n",
    "    \"member_id\",\n",
    "    \"test_source\",\n",
    "    \"test_name\",\n",
    "    \"patient_city\",\n",
    "    \"patient_state\",\n",
    "    \"test_date\",\n",
    "    # Masking the phone number using the custom Python function\n",
    "    mask_phone_number_udf(\"patient_phone\").alias(\"masked_phone_number\"),\n",
    "    \n",
    "    \"created_date\"\n",
    ")\n",
    "\n",
    "\n",
    "display(silver_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd1ba21-005c-40a8-ab71-9e5719d6634b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_df.write.mode(\"overwrite\").saveAsTable(\"lab_data_silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e7c1ecb6-2c1c-49db-8465-ffa2d9f9fb65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Import necessary functions\n",
    "# from pyspark.sql.functions import col, array\n",
    "# from functools import reduce, lru_cache,\n",
    "\n",
    "# # Create an example DataFrame\n",
    "# data = [\n",
    "#     (1, \"source1\", \"test1\", \"city1\", \"state1\", \"2025-05-24\", \"1234567890\", \"2025-05-24\"),\n",
    "#     (2, \"source2\", \"test2\", \"city2\", \"state2\", \"2025-05-24\", None, \"2025-05-24\"),\n",
    "#     (3, None, \"test3\", \"city3\", \"state3\", \"2025-05-24\", \"0987654321\", \"2025-05-24\")\n",
    "# ]\n",
    "# columns = [\"member_id\", \"test_source\", \"test_name\", \"patient_city\", \"patient_state\", \"test_date\", \"patient_phone\", \"created_date\"]\n",
    "# df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# # List of columns to check for non-null values\n",
    "# columns_to_check = [\"member_id\", \"test_source\", \"test_name\", \"patient_city\", \"patient_state\", \"test_date\", \"patient_phone\", \"created_date\"]\n",
    "\n",
    "# # Create a list of conditions where each column in 'columns_to_check' is checked for non-null values\n",
    "# conditions = [col(c).isNotNull() for c in columns_to_check]\n",
    "\n",
    "# print(\"Condition \\n\",conditions)\n",
    "\n",
    "# # Combine all conditions using a logical AND operation\n",
    "# combined_condition = reduce(lambda a, b: a & b, conditions)\n",
    "\n",
    "\n",
    "# print(\"Combined Condition \\n\",combined_condition)\n",
    "\n",
    "# # Filter the DataFrame using the combined condition\n",
    "# filtered_df = df.filter(combined_condition)\n",
    "\n",
    "# # Display the filtered DataFrame\n",
    "# display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccd1bbae-b1a0-4d03-8ed5-8b75c720e9b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM lab_data_silver"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7801631232272122,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_lab_data_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
