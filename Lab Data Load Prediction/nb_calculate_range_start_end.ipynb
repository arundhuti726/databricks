{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0c47c9-233b-4356-8755-a5035045e11b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql('USE CATALOG dev_edh')\n",
    "spark.sql('USE SCHEMA dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "817aab1b-0cca-4831-8988-521793cb2234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--select * from dev_edh.dummy.lab_data_count_gold order by test_date desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ebd76baf-f4dc-4d55-9a09-ea8e18cfd925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Lab_name = dbutils.jobs.taskValues.get(\n",
    "#     \"range_count\", \"Lab_name\",\n",
    "#     debugValue=\"default_lab_name\"\n",
    "# );print(Lab_name)\n",
    "\n",
    "# lab = dbutils.jobs.taskValues.get(taskKey = \"get_Lab_Name\", key = \"Lab_name\", debugValue = \"Lab A\")\n",
    "\n",
    "# print(f\"Received value: {lab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddc22c65-42ba-4648-a96d-aa87afc7d70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Static mapping for this example\n",
    "lab_schedule = spark.createDataFrame([\n",
    "    (\"Lab A\", \"weekly\"),\n",
    "    (\"Lab B\", \"monthly\"),\n",
    "    (\"Lab D\", \"weekly\")\n",
    "], [\"lab_name\", \"frequency\"])\n",
    "\n",
    "lab_schedule.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f2ad400-2681-4f3d-96b8-f4c21e98a5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crnt_load_look_back = {\"weekly\": 7, \"monthly\": 30, \"daily\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9d8f6c98-e518-42e9-a9c0-68fb1b1e5598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Date from the tables max\n",
    "-- Select max(test_date) as Date from dev_edh.dummy.lab_data_count_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "47749d3e-8d02-410a-83d5-1cbb83a5a7c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--CREATE WIDGET DROPDOWN Lab DEFAULT \"Lab A\" CHOICES SELECT * FROM (VALUES (\"Lab A\"), (\"Lab B\"), (\"Lab D\"));\n",
    "--CREATE WIDGET DROPDOWN Date DEFAULT '2025-01-01' CHOICES select explode(sequence(DATE'2024-03-23',DATE'2025-12-31',INTERVAL 1 DAY )) ORDER BY col DESC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6861932f-8e79-4ade-bd42-add3467ff83d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#print(dbutils.widgets.get(\"Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f571161b-3456-4f43-8690-45cdd95c1f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# getting the lab's name and Date\n",
    "lab = spark.sql('select distinct test_source from dev_edh.dummy.lab_data_count_gold').collect()[0][0];\n",
    "key_date = spark.sql('Select max(test_date) as Date from dev_edh.dummy.lab_data_count_gold').collect()[0][0];\n",
    "\n",
    "display(key_date,lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e944c7-1eac-4d12-8086-b78a19dbb556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#lab = dbutils.widgets.get(\"Lab\")\n",
    "frequency = lab_schedule.filter(F.col(\"lab_name\") == lab).select(\"frequency\").collect()[0][0]\n",
    "crnt_look_back_day = crnt_load_look_back[frequency]\n",
    "print(lab,frequency)\n",
    "print(crnt_look_back_day)\n",
    "#print(dbutils.widgets.get(\"Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a722806-7ed4-4956-859f-775c90cc2fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# change bucket\n",
    "if frequency == \"weekly\":\n",
    "    bucket = 42\n",
    "elif frequency == \"daily\":\n",
    "    bucket = 30\n",
    "elif frequency == \"monthly\":\n",
    "    bucket = 180\n",
    "# Date value from the previous SQL tab to here as python value\n",
    "\n",
    "#key_date = dbutils.widgets.get(\"Date\")\n",
    "#lab = dbutils.widgets.get(\"Lab\")\n",
    "look_back_days = bucket+1\n",
    "\n",
    "# run_date = pd.to_datetime(\n",
    "#     pd.to_datetime(key_date) - pd.DateOffset(days=bucket), format=\"%Y-%m-%d\"\n",
    "# )   # We can change it in poroduction by taking the date from system\n",
    "\n",
    "# Add 1 day on the run day\n",
    "run_date = pd.to_datetime(pd.to_datetime(key_date) + pd.DateOffset(days=1), format=\"%Y-%m-%d\")\n",
    "print(key_date, look_back_days)\n",
    "print('run_date-',run_date)\n",
    "current_end_dt = pd.to_datetime(key_date) \n",
    "    # current_end_dt = pd.to_datetime(pd.to_datetime(key_date) - pd.DateOffset(days=1), format=\"%Y-%m-%d\") \n",
    "print('current_end_dt -',current_end_dt)\n",
    "\n",
    "current_start_dt = pd.to_datetime(\n",
    "    pd.to_datetime(key_date) - pd.DateOffset(days=crnt_look_back_day ), format=\"%Y-%m-%d\"\n",
    ")\n",
    "print('current_start_dt -',current_start_dt)\n",
    "lookback_end_dt = pd.to_datetime(\n",
    "    pd.to_datetime(current_start_dt) - pd.DateOffset(days=1), format=\"%Y-%m-%d\"\n",
    ")\n",
    "lookback_start_dt = pd.to_datetime(\n",
    "    pd.to_datetime(current_start_dt) - pd.DateOffset(days=look_back_days),\n",
    "    format=\"%Y-%m-%d\",\n",
    ")\n",
    "\n",
    "spark.conf.set(\"key.date\", key_date)\n",
    "key_date = datetime.strptime(key_date, '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "spark.conf.set(\"key.date\", key_date)\n",
    "#display(key_date)\n",
    "\n",
    "spark.conf.set(\"look.back.days\", look_back_days)\n",
    "# # assign values for pyspark\n",
    "current_end_dt = str(current_end_dt)\n",
    "current_start_dt = str(current_start_dt)\n",
    "lookback_end_dt = str(lookback_end_dt)\n",
    "lookback_start_dt = str(lookback_start_dt)\n",
    "bucket = str(bucket)\n",
    "spark.conf.set(\"current.end.date\", current_end_dt)\n",
    "spark.conf.set(\"current.start.date\", current_start_dt)\n",
    "spark.conf.set(\"lookback.end.date\", lookback_end_dt)\n",
    "spark.conf.set(\"lookback.start.date\", lookback_start_dt)\n",
    "spark.conf.set(\"week.days\", crnt_look_back_day)\n",
    "spark.conf.set(\"test.source.name\", lab)\n",
    "spark.conf.set(\"lab.bucket\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0972917-d74c-4325-bcf8-49c765d10984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Change format and print just to check\n",
    "CREATE OR REPLACE TEMPORARY VIEW temp_view AS\n",
    "select\n",
    "'${test.source.name}' as lab_name,\n",
    "'${key.date}' as key_date_entered,\n",
    "  '${look.back.days}' as number_of_days_back,\n",
    "  cast('${current.end.date}' as date) as current_end_dt,\n",
    "  cast('${current.start.date}' as date) as current_start_dt,\n",
    "  cast('${lookback.end.date}' as date) as lookback_end_dt,\n",
    "  cast('${lookback.start.date}' as date) as lookback_start_dt,\n",
    "  '${week.days}' as crnt_look_back_day;\n",
    "\n",
    "select * FROM temp_view\n",
    "  --select key_date_entered from temp_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f940a3-823e-4ce4-83fc-9035b8e0eded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Current load count\n",
    "create or replace temp view current_data_load as\n",
    "\n",
    "select distinct test_source,cast('${current.start.date}' as date) as current_st_dt, cast('${current.end.date}' as date) as current_end_dt,sum(total_load) as total_load\n",
    "from dev_edh.dummy.lab_data_count_gold\n",
    "where test_date between '${current.start.date}' and '${current.end.date}' \n",
    "and test_source = '${test.source.name}'\n",
    "group by test_source;\n",
    "\n",
    "select * from current_data_load \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acc2220b-921e-4bba-9274-7b31fc846ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Lookback load count\n",
    "create or replace temp view lookback_data_load as\n",
    "\n",
    "select test_source,\n",
    "  cast(date_trunc('month', test_date) as date) as month,\n",
    "  sum(total_load) as monthly_load\n",
    "from dev_edh.dummy.lab_data_count_gold\n",
    "where test_date between '${lookback.start.date}' and '${lookback.end.date}' \n",
    "and test_source = '${test.source.name}'\n",
    "group by test_source, date_trunc('month', test_date);\n",
    "\n",
    "-- Calculate Range start and Range end value from lookback data\n",
    "create or replace temp view range_start_end as\n",
    "select round(avg(monthly_load) - stddev(monthly_load)) as range_start,\n",
    "       round(avg(monthly_load) + stddev(monthly_load)) as range_end,\n",
    "       min(month) as lookback_start_date, max(month) as lookback_end_date, '${test.source.name}'as test_source\n",
    "from lookback_data_load;\n",
    "\n",
    "select * from range_start_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c7aaf8-303b-485b-a527-5c05d9820f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Join the 2 temp view 'range_start_end' and 'current_data_load'\n",
    "create or replace temp view current_data_status as\n",
    "select t1.test_source,t1.lookback_start_date,t1.lookback_end_date,t2.current_st_dt,t2.current_end_dt,t1.range_start,t1.range_end,t2.total_load,\n",
    "case when t2.total_load between t1.range_start and t1.range_end then 'within range' else 'out of range' end as status\n",
    "from range_start_end t1\n",
    "join current_data_load t2 \n",
    "on t1.test_source = t2.test_source;\n",
    "\n",
    "select * from current_data_status\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "98c517b9-e466-4fb5-ad0c-93f596562211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- calculate load for each month dynamically based on the frequency  [this is working as expected , but I changed this to temp view]\n",
    "# with TEMP as(\n",
    "# select distinct * from(\n",
    "# select \n",
    "#   test_source,\n",
    "#   cast(date_trunc('month', test_date) as date) as dates,\n",
    "#   sum(total_load) over (partition by test_source, date_trunc('month', test_date)) as monthly_load\n",
    "# from dev_edh.dummy.lab_data_count_gold\n",
    "# where test_source = '${test.source.name}' and test_date between '${lookback.start.date}' and '${lookback.end.date}'\n",
    "# group by test_source, date_trunc('month', test_date), test_date,total_load)\n",
    "# )\n",
    "# select round(avg(monthly_load) - stddev(monthly_load))  as range_start,round(avg(monthly_load) + stddev(monthly_load)) as range_end from TEMP\n",
    "# group by test_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c7e5c4-abe4-45da-991a-cd55493107e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create table if not exists dev_edh.dummy.lab_data_status (\n",
    "  test_source string,\n",
    "  range_start double,\n",
    "  range_end double,\n",
    "  current_load bigint,\n",
    "  load_start_date date,\n",
    "  load_end_date date,\n",
    "  current_start_date date,\n",
    "  current_end_date date,\n",
    "  status string,\n",
    "  timestamp timestamp\n",
    ");\n",
    "\n",
    "-- insert into dev_edh.dummy.lab_data_status\n",
    "-- select \n",
    "--   current_load.test_source,\n",
    "--   range_start,\n",
    "--   range_end,\n",
    "--   total_load as current_load,\n",
    "--   cast('${lookback.start.date}' as date) as load_start_date,\n",
    "--   cast('${lookback.end.date}' as date) as load_end_date,\n",
    "--   cast('${current.start.date}' as date) as current_start_date,\n",
    "--   cast('${current.end.date}' as date) as current_end_date,\n",
    "--   case \n",
    "--     when total_load between range_start and range_end then 'within range' \n",
    "--     else 'out of range' \n",
    "--   end as status,\n",
    "--   current_timestamp() as timestamp      \n",
    "-- from (\n",
    "--   select \n",
    "--     test_source,\n",
    "--     count(*) as total_load\n",
    "--   from dev_edh.dummy.lab_data_count_gold\n",
    "--   where test_source = '${test.source.name}' and test_date between '${current.start.date}' and '${current.end.date}'\n",
    "--   group by test_source\n",
    "-- ) as current_load\n",
    "-- cross join (\n",
    "--   select \n",
    "--     test_source,\n",
    "--     round(avg(monthly_load) - stddev(monthly_load)) as range_start,\n",
    "--     round(avg(monthly_load) + stddev(monthly_load)) as range_end\n",
    "--   from (\n",
    "--     select \n",
    "--       test_source,\n",
    "--       cast(date_trunc('month', test_date) as date) as dates,\n",
    "--       count(*) over (partition by test_source, date_trunc('month', test_date)) as monthly_load\n",
    "--     from dev_edh.dummy.lab_data_count_gold\n",
    "--     where test_source = '${test.source.name}' and test_date between '${lookback.start.date}' and '${lookback.end.date}'\n",
    "--     group by test_source, date_trunc('month', test_date), test_date\n",
    "--   ) as TEMP\n",
    "--   group by test_source\n",
    "-- ) as range_values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1278fa-c949-4f1d-8541-a2b619dd0519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--create a merge statement and update only into the table 'lab_data_status' if and only if the test_source, lookback_start_date, lookback_end_date, current_start_date, current_end_date, are different otherwise insert a new row using the temp view current_data_status\n",
    "MERGE INTO dev_edh.dummy.lab_data_status AS target\n",
    "USING current_data_status AS source\n",
    "ON target.test_source = source.test_source \n",
    "AND target.load_start_date = source.lookback_start_date \n",
    "AND target.load_end_date = source.lookback_end_date \n",
    "AND target.current_start_date = source.current_st_dt \n",
    "AND target.current_end_date = source.current_end_dt\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    target.range_start = source.range_start,\n",
    "    target.range_end = source.range_end,\n",
    "    target.current_load = source.total_load,\n",
    "    target.status = source.status,\n",
    "    target.timestamp = current_timestamp()\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (test_source, range_start, range_end, current_load, load_start_date, load_end_date, current_start_date, current_end_date, status, timestamp)\n",
    "  VALUES (source.test_source, source.range_start, source.range_end, source.total_load, source.lookback_start_date, source.lookback_end_date, source.current_st_dt, source.current_end_dt, source.status, current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ed614b66-b607-4941-955e-47a0d12fbcca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# MERGE INTO dev_edh.dummy.lab_data_status AS target\n",
    "# USING (\n",
    "#   SELECT \n",
    "#     current_load.test_source,\n",
    "#     range_start,\n",
    "#     range_end,\n",
    "#     total_load AS current_load,\n",
    "#     CAST('${lookback_start_dt}' AS DATE) AS load_start_date,\n",
    "#     CAST('${lookback_end_dt}' AS DATE) AS load_end_date,\n",
    "#     CAST('${current_start_dt}' AS DATE) AS current_start_date,\n",
    "#     CAST('${current_end_dt}' AS DATE) AS current_end_date,\n",
    "#     CASE \n",
    "#       WHEN total_load BETWEEN range_start AND range_end THEN 'within range' \n",
    "#       ELSE 'out of range' \n",
    "#     END AS status,\n",
    "#     current_timestamp() AS timestamp      \n",
    "#   FROM (\n",
    "#     SELECT \n",
    "#       test_source,\n",
    "#       COUNT(*) AS total_load\n",
    "#     FROM dev_edh.dummy.lab_data_count_gold\n",
    "#     WHERE test_source = '${lab}' AND test_date BETWEEN '${current_start_dt}' AND '${current_end_dt}'\n",
    "#     GROUP BY test_source\n",
    "#   ) AS current_load\n",
    "#   CROSS JOIN (\n",
    "#     SELECT \n",
    "#       test_source,\n",
    "#       ROUND(AVG(monthly_load) - STDDEV(monthly_load)) AS range_start,\n",
    "#       ROUND(AVG(monthly_load) + STDDEV(monthly_load)) AS range_end\n",
    "#     FROM (\n",
    "#       SELECT \n",
    "#         test_source,\n",
    "#         CAST(DATE_TRUNC('month', test_date) AS DATE) AS dates,\n",
    "#         COUNT(*) OVER (PARTITION BY test_source, DATE_TRUNC('month', test_date)) AS monthly_load\n",
    "#       FROM dev_edh.dummy.lab_data_count_gold\n",
    "#       WHERE test_source = '${lab}' AND test_date BETWEEN '${lookback_start_dt}' AND '${lookback_end_dt}'\n",
    "#       GROUP BY test_source, DATE_TRUNC('month', test_date), test_date\n",
    "#     ) AS TEMP\n",
    "#     GROUP BY test_source\n",
    "#   ) AS range_values\n",
    "# ) AS source\n",
    "# ON target.test_source = source.test_source\n",
    "#   AND target.load_start_date = source.load_start_date\n",
    "#   AND target.load_end_date = source.load_end_date\n",
    "#   AND target.current_start_date = source.current_start_date\n",
    "#   AND target.current_end_date = source.current_end_date\n",
    "# WHEN MATCHED THEN\n",
    "#   UPDATE SET\n",
    "#     target.range_start = source.range_start,\n",
    "#     target.range_end = source.range_end,\n",
    "#     target.current_load = source.current_load,\n",
    "#     target.status = source.status,\n",
    "#     target.timestamp = source.timestamp\n",
    "# WHEN NOT MATCHED THEN\n",
    "#   INSERT (test_source, range_start, range_end, current_load, load_start_date, load_end_date, current_start_date, current_end_date, status, timestamp)\n",
    "#   VALUES (source.test_source, source.range_start, source.range_end, source.current_load, source.load_start_date, source.load_end_date, source.current_start_date, source.current_end_date, source.status, source.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1c18170f-a655-431f-97b1-0b73452c10d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update the table lab_data_status add a field called timestamp. Which will store the running timestamp\n",
    "# spark.sql(\"\"\"\n",
    "# ALTER TABLE dev_edh.dummy.lab_data_status\n",
    "# ADD COLUMNS (timestamp TIMESTAMP)\n",
    "# \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030f3a15-266a-4f27-be1a-f6e0a580cd37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM dev_edh.dummy.lab_data_status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f45d9db-8cbc-41fc-ab8c-2e01bc334711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Remove widgets lab\n",
    "#dbutils.widgets.remove(\"Date\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4615693316898580,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_calculate_range_start_end",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
